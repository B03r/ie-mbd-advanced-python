{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![IE](../img/ie.png)\n",
    "\n",
    "# Extra: Big Data Science in Python with Dask\n",
    "\n",
    "### Juan Luis Cano Rodr√≠guez <jcano@faculty.ie.edu> - Master in Business Analytics and Big Data (2020-06-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Larger than RAM `DataFrame`s \n",
    "\n",
    "A decade ago, **pandas** brought a revolution by enabling Python users to perform data analysis and manipulation in desktop computers and laptops. However, by that time CPU speed evolution had already slowed down dramatically, and technologies like Hadoop had appeared to enable manipulation of massive datasets (\"Big Data\").\n",
    "\n",
    "![Clock speeds](../img/clock.jpg)\n",
    "\n",
    "Essentially two axis of improvement exist to accelerate the performance of data analysis pipelines:\n",
    "\n",
    "1. **Scale up**: Better machines, faster CPUs. Incremental progress over the latest years, but no longer exponential growth (Moore's law)\n",
    "2. **Scale out**: More machines, parallel processing, GPUs, TPUs. More complexity in heterogeneous setups, not all tasks can be easily parallelized.\n",
    "\n",
    "![Scale](../img/scale.png)\n",
    "\n",
    "The ecosystem of solutions that try to improve data analysis workflows in Python is vast, both for scaling up and scaling out. In this session we will study Dask, a Python library and ecosystem that leverages existing APIs (NumPy, pandas) and extends them to support parallel computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask\n",
    "\n",
    "<img src=\"../img/dask.svg\" width=\"300px\" />\n",
    "\n",
    "From the documentation:\n",
    "\n",
    "> Dask is a flexible library for parallel computing in Python.\n",
    ">\n",
    "> Dask emphasizes the following virtues:\n",
    "> \n",
    "> - **Familiar**: Provides parallelized NumPy array and Pandas DataFrame objects\n",
    "> - **Flexible**: Provides a task scheduling interface for more custom workloads and integration with other projects.\n",
    "> - **Native**: Enables distributed computing in pure Python with access to the PyData stack.\n",
    "> - **Fast**: Operates with low overhead, low latency, and minimal serialization necessary for fast numerical algorithms\n",
    "> - **Scales up**: Runs resiliently on clusters with 1000s of cores\n",
    "> - **Scales down**: Trivial to set up and run on a laptop in a single process\n",
    "> - **Responsive**: Designed with interactive computing in mind, it provides rapid feedback and diagnostics to aid humans\n",
    "\n",
    "![Dask overview](../img/dask-overview.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Installation is trivially simple. With conda:\n",
    "\n",
    "```\n",
    "$ conda install dask\n",
    "```\n",
    "\n",
    "or, alternatively, with pip:\n",
    "\n",
    "```\n",
    "$ pip install dask[complete]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy evaluation\n",
    "\n",
    "Let's do a simple example with `dask.array` to understand how lazy evaluation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to perform any operation on this array, it does not execute immediately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, Dask builds a task graph with all the necessary operations and its dependencies so we can visualize it and reason about it. This graph is stored in common Python data structures like dicts, lists, tuples and arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to actually perform the operation, we need to call `.compute()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the same API we use to transform our Dask array into a normal NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: NYC taxis\n",
    "\n",
    "Apart from `dask.array` mimicking the NumPy API, we also have `dask.dataframe` that behaves like pandas DataFrames.\n",
    "\n",
    "<img src=\"../img/dask-dataframe.svg\" width=\"300px\" />\n",
    "\n",
    "To study how do they work, we will use a subset of the NYC taxi dataset:\n",
    "\n",
    "```\n",
    "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-01.csv\n",
    "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-02.csv\n",
    "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-03.csv\n",
    "https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2019-04.csv\n",
    "```\n",
    "\n",
    "(see https://github.com/toddwschneider/nyc-taxi-data for more info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "These will be the packages we will need:\n",
    "\n",
    "```\n",
    "$ conda install dask python-graphviz ipywidgets notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du ../data/yellow*.csv -h -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_types = {\n",
    "    1: \"Credit Card\",\n",
    "    2: \"Cash\",\n",
    "    3: \"No Charge\",\n",
    "    4: \"Dispute\",\n",
    "    5: \"Unknown\",\n",
    "    6: \"Voided trip\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Create a new `tip_fraction` column with the tip amount divided by the fare amount (_note: use only nonzero values of both to avoid division errors and biased results_)\n",
    "2. Group the dataframe by pickup hour, and compute the mean `tip_fraction` by pickup hour\n",
    "3. Visualize the mean `tip_fraction` by pickup hour in a line plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this warning mean? In Dask, some operations are partition-sensitive https://docs.dask.org/en/latest/dataframe-design.html#partitions. Some of them will give us a warning, and some others will fail directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">This will load all the data in RAM! Do not run it if you are in a constrained environment</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_mean.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
